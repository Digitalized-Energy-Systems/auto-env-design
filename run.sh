

# Hyperparameter optimization experiments (sections 6 and 7 of the paper)
python optuna_experiment.py --agent "Ddpg" --enviro "opfgym.envs:LoadShedding" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241128_multi_GA_reduced/load/" --num 100 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'reward_function': 'parameterized', 'train_data': 'mixed'}"
python optuna_experiment.py --agent "Ddpg" --enviro "opfgym.envs:EcoDispatch" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241128_multi_GA_reduced/eco/" --num 100 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'reward_function': 'parameterized', 'train_data': 'mixed'}"
python optuna_experiment.py --agent "Ddpg" --enviro "opfgym.envs:VoltageControl" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241128_multi_GA_reduced/voltage/" --num 100 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'reward_function': 'parameterized', 'train_data': 'mixed'}"
python optuna_experiment.py --agent "Ddpg" --enviro "opfgym.envs:QMarket" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241128_multi_GA_reduced/qmarket/" --num 100 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'reward_function': 'parameterized', 'train_data': 'mixed'}"
python optuna_experiment.py --agent "Ddpg" --enviro "opfgym.envs:MaxRenewable" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241128_multi_GA_reduced/renewable/" --num 100 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'reward_function': 'parameterized', 'train_data': 'mixed'}"


# Baseline experiments for hyperparameter optimization (replace penalty_weight accordingly)
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:LoadShedding" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241203_baseline/load_09/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'train_data': 'simbench', 'reward_scaling': 'normalization', 'reward_function_params': {'penalty_weight': 0.9}}"
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:EcoDispatch" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241203_baseline/eco_09/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'train_data': 'simbench', 'reward_scaling': 'normalization', 'reward_function_params': {'penalty_weight': 0.9}}"
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:VoltageControl" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241203_baseline/voltage_09/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'train_data': 'simbench', 'reward_scaling': 'normalization', 'reward_function_params': {'penalty_weight': 0.9}}"
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:QMarket" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241203_baseline/qmarket_09/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'train_data': 'simbench', 'reward_scaling': 'normalization', 'reward_function_params': {'penalty_weight': 0.9}}"
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:MaxRenewable" --store --steps 40000 --test-epi 500 --test-steps 99999  --test-inter "25000, 30000, 35000, 40000" --path "data/20241203_baseline/renewable_09/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 100000}" --env-hyper "{'random_validation_steps': True, 'validation_share': 0.6, 'train_data': 'simbench', 'reward_scaling': 'normalization', 'reward_function_params': {'penalty_weight': 0.9}}"


# Verification experiments (section 8)
## Apply SAC to the Voltage Control environment with the **best** design
python single_experiment.py --agent "Sac" --enviro "opfgym.envs:VoltageControl" --store --steps 500000 --test-epi 500 --test-steps 99999  --test-inter "50000" --path "data/20241223_best_design_500k/voltage_sac_best_alldata/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 500000}" --env-hyper "{'reward_function_params': {'reward_scaling': 'normalization', 'invalid_penalty': 0.5655, 'invalid_objective_share': 0.4659, 'penalty_weight': 0.1567, 'valid_reward': 0.972}, 'constraint_params': {'only_worst_case_violations': False, 'penalty_power': 1.0}, 'sampling_params': {'data_probabilities': (0.357951156362772, 0.6449123584232672, 1.0), 'noise_factor': 0.0667, 'interpolate_steps': False}, 'clipped_action_penalty': 4.0246, 'invalid_penalty': 0.5655, 'penalty_weight': 0.1567, 'valid_reward': 0.972, 'add_ext_grid_power': False, 'add_line_loading': True, 'add_mean_obs': False, 'add_trafo_loading': True, 'add_voltage_angle': False, 'add_voltage_magnitude': False, 'autoscale_actions': True, 'diff_objective': True, 'evaluate_on': 'test', 'random_validation_steps': True, 'reward_function': 'parameterized'}"

## Same for EcoDispatch
python single_experiment.py --agent "Sac" --enviro "opfgym.envs:EcoDispatch" --store --steps 500000 --test-epi 500 --test-steps 99999  --test-inter "50000" --path "data/20241223_best_design_500k/eco_sac_best_alldata/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 500000}" --env-hyper "{'reward_function_params': {'reward_scaling': 'normalization', 'invalid_penalty': 1.1141, 'invalid_objective_share': 0.7987, 'penalty_weight': 0.5449, 'valid_reward': 0.8793}, 'constraint_params': {'only_worst_case_violations': False, 'penalty_power': 1.0}, 'sampling_params': {'data_probabilities': (0.3496733145048519, 0.7620533858723753, 1.0), 'noise_factor': 0.099, 'interpolate_steps': False}, 'clipped_action_penalty': 3.975, 'invalid_penalty': 1.1141, 'penalty_weight': 0.5449, 'valid_reward': 0.8793, 'add_ext_grid_power': False, 'add_line_loading': True, 'add_mean_obs': False, 'add_trafo_loading': True, 'add_voltage_angle': True, 'add_voltage_magnitude': False, 'autoscale_actions': True, 'diff_objective': True, 'evaluate_on': 'test', 'random_validation_steps': True, 'reward_function': 'parameterized'}"

## Same for DDPG (VoltageControl)
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:VoltageControl" --store --steps 500000 --test-epi 500 --test-steps 99999  --test-inter "50000" --path "data/20241223_best_design_500k/voltage_ddpg_best_alldata/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 500000}" --env-hyper "{'reward_function_params': {'reward_scaling': 'normalization', 'invalid_penalty': 0.5655, 'invalid_objective_share': 0.4659, 'penalty_weight': 0.1567, 'valid_reward': 0.972}, 'constraint_params': {'only_worst_case_violations': False, 'penalty_power': 1.0}, 'sampling_params': {'data_probabilities': (0.357951156362772, 0.6449123584232672, 1.0), 'noise_factor': 0.0667, 'interpolate_steps': False}, 'clipped_action_penalty': 4.0246, 'invalid_penalty': 0.5655, 'penalty_weight': 0.1567, 'valid_reward': 0.972, 'add_ext_grid_power': False, 'add_line_loading': True, 'add_mean_obs': False, 'add_trafo_loading': True, 'add_voltage_angle': False, 'add_voltage_magnitude': False, 'autoscale_actions': True, 'diff_objective': True, 'evaluate_on': 'test', 'random_validation_steps': True, 'reward_function': 'parameterized'}"

## Same for DDPG (EcoDispatch)
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:EcoDispatch" --store --steps 500000 --test-epi 500 --test-steps 99999  --test-inter "50000" --path "data/20241223_best_design_500k/eco_ddpg_best_alldata/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 500000}" --env-hyper "{'reward_function_params': {'reward_scaling': 'normalization', 'invalid_penalty': 1.1141, 'invalid_objective_share': 0.7987, 'penalty_weight': 0.5449, 'valid_reward': 0.8793}, 'constraint_params': {'only_worst_case_violations': False, 'penalty_power': 1.0}, 'sampling_params': {'data_probabilities': (0.3496733145048519, 0.7620533858723753, 1.0), 'noise_factor': 0.099, 'interpolate_steps': False}, 'clipped_action_penalty': 3.975, 'invalid_penalty': 1.1141, 'penalty_weight': 0.5449, 'valid_reward': 0.8793, 'add_ext_grid_power': False, 'add_line_loading': True, 'add_mean_obs': False, 'add_trafo_loading': True, 'add_voltage_angle': True, 'add_voltage_magnitude': False, 'autoscale_actions': True, 'diff_objective': True, 'evaluate_on': 'test', 'random_validation_steps': True, 'reward_function': 'parameterized'}"


## Apply SAC to the Voltage Control environment with the **base** design
python single_experiment.py --agent "Sac" --enviro "opfgym.envs:VoltageControl" --store --steps 500000 --test-epi 500 --test-steps 99999  --test-inter "50000" --path "data/20241223_best_design_500k/voltage_sac_base_alldata/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 500000}" --env-hyper "{'reward_function_params': {'reward_scaling': 'normalization', 'invalid_penalty': 0.0, 'invalid_objective_share': 1.0, 'penalty_weight': 0.5, 'valid_reward': 0.0}, 'constraint_params': {'only_worst_case_violations': False, 'penalty_power': 1.0}, 'sampling_params': {'data_probabilities': (1.0, 1.0, 1.0), 'noise_factor': 0.0, 'interpolate_steps': False}, 'clipped_action_penalty': 0.0, 'invalid_penalty': 0.0, 'penalty_weight': 0.5, 'valid_reward': 0.0, 'add_ext_grid_power': False, 'add_line_loading': False, 'add_mean_obs': False, 'add_trafo_loading': False, 'add_voltage_angle': False, 'add_voltage_magnitude': False, 'autoscale_actions': True, 'diff_objective': False, 'evaluate_on': 'test', 'random_validation_steps': True, 'reward_function': 'parameterized'}"

## Same for EcoDispatch
python single_experiment.py --agent "Sac" --enviro "opfgym.envs:EcoDispatch" --store --steps 500000 --test-epi 500 --test-steps 99999  --test-inter "50000" --path "data/20241223_best_design_500k/eco_sac_base_alldata/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 500000}" --env-hyper "{'reward_function_params': {'reward_scaling': 'normalization', 'invalid_penalty': 0.0, 'invalid_objective_share': 1.0, 'penalty_weight': 0.1, 'valid_reward': 0.0}, 'constraint_params': {'only_worst_case_violations': False, 'penalty_power': 1.0}, 'sampling_params': {'data_probabilities': (1.0, 1.0, 1.0), 'noise_factor': 0.0, 'interpolate_steps': False}, 'clipped_action_penalty': 0.0, 'invalid_penalty': 0.0, 'penalty_weight': 0.1, 'valid_reward': 0.0, 'add_ext_grid_power': False, 'add_line_loading': False, 'add_mean_obs': False, 'add_trafo_loading': False, 'add_voltage_angle': False, 'add_voltage_magnitude': False, 'autoscale_actions': True, 'diff_objective': False, 'evaluate_on': 'test', 'random_validation_steps': True, 'reward_function': 'parameterized'}"

## Same for DDPG (VoltageControl)
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:VoltageControl" --store --steps 500000 --test-epi 500 --test-steps 99999  --test-inter "50000" --path "data/20241223_best_design_500k/voltage_ddpg_base_alldata/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 500000}" --env-hyper "{'reward_function_params': {'reward_scaling': 'normalization', 'invalid_penalty': 0.0, 'invalid_objective_share': 1.0, 'penalty_weight': 0.5, 'valid_reward': 0.0}, 'constraint_params': {'only_worst_case_violations': False, 'penalty_power': 1.0}, 'sampling_params': {'data_probabilities': (1.0, 1.0, 1.0), 'noise_factor': 0.0, 'interpolate_steps': False}, 'clipped_action_penalty': 0.0, 'invalid_penalty': 0.0, 'penalty_weight': 0.5, 'valid_reward': 0.0, 'add_ext_grid_power': False, 'add_line_loading': False, 'add_mean_obs': False, 'add_trafo_loading': False, 'add_voltage_angle': False, 'add_voltage_magnitude': False, 'autoscale_actions': True, 'diff_objective': False, 'evaluate_on': 'test', 'random_validation_steps': True, 'reward_function': 'parameterized'}"

## Same for DDPG (EcoDispatch)
python single_experiment.py --agent "Ddpg" --enviro "opfgym.envs:EcoDispatch" --store --steps 500000 --test-epi 500 --test-steps 99999  --test-inter "50000" --path "data/20241223_best_design_500k/eco_ddpg_base_alldata/" --num 10 --hyper "{'gamma': 0.9, 'memory_size': 500000}" --env-hyper "{'reward_function_params': {'reward_scaling': 'normalization', 'invalid_penalty': 0.0, 'invalid_objective_share': 1.0, 'penalty_weight': 0.1, 'valid_reward': 0.0}, 'constraint_params': {'only_worst_case_violations': False, 'penalty_power': 1.0}, 'sampling_params': {'data_probabilities': (1.0, 1.0, 1.0), 'noise_factor': 0.0, 'interpolate_steps': False}, 'clipped_action_penalty': 0.0, 'invalid_penalty': 0.0, 'penalty_weight': 0.1, 'valid_reward': 0.0, 'add_ext_grid_power': False, 'add_line_loading': False, 'add_mean_obs': False, 'add_trafo_loading': False, 'add_voltage_angle': False, 'add_voltage_magnitude': False, 'autoscale_actions': True, 'diff_objective': False, 'evaluate_on': 'test', 'random_validation_steps': True, 'reward_function': 'parameterized'}"
